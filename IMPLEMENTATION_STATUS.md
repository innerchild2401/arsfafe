# Implementation Status: Conversation Memory, Streaming, and Map-Reduce

## ‚úÖ IMPLEMENTED

### 1. Conversation Memory (The "Amnesia" Problem) ‚úÖ

**Status:** Fully Implemented

**What Was Added:**
- `get_conversation_history()` function: Fetches last 3 turn pairs (6 messages) from chat history
- `build_conversation_context()` function: Formats messages as "User: ... / Assistant: ..."
- `rewrite_query_with_context()` function: Uses GPT-4o-mini to de-reference pronouns and contextual references
- Conversation history injection: All three paths (A, B, C) now inject conversation history into prompts

**How It Works:**
1. **Fetch History:** Before processing, fetch last 6 messages (3 turn pairs) ordered by created_at DESC
2. **Query Rewrite:** Run fast rewrite using GPT-4o-mini:
   - Input: History + New Question ("What happens if we miss that date?")
   - Output: "Consequences of missing the $5M bond maturity date in October."
3. **Search with Rewritten Query:** Use rewritten query for embedding generation and keyword search
4. **Inject History:** Add conversation context to system prompt: "Previous conversation context: ..."

**Integration Points:**
- Path A (Hybrid Search): Uses rewritten query for search, history in prompt
- Path B (Summaries): History in prompt (for context-aware summaries)
- Path C (Deep Reasoner): Uses rewritten query for search, history in prompt for deep reasoning

**Files Modified:**
- `backend/app/routers/chat.py`: Added helper functions, query rewrite, history injection

---

### 2. Map-Reduce for Multi-Book Path C (The "Library" Problem) ‚úÖ

**Status:** Fully Implemented

**What Was Added:**
- Map-Reduce detection: Detects multi-book compare/analyze queries
- Sequential searches per book: Searches each book separately (top 5 chunks per book)
- Synthesis prompt: Instructs GPT-4o to explicitly contrast information across books
- Table format suggestion: Prompts LLM to use table format for structured comparisons

**How It Works:**
1. **Detection:** If `book_id` is NULL (multi-book) AND query contains compare keywords ‚Üí trigger Map-Reduce
2. **MAP Phase:** For each book:
   - Search "Risk factors" filtered to that specific book
   - Retrieve top 5 chunks per book
   - Store in `book_chunks_map[book_id] = chunks`
3. **REDUCE Phase:** 
   - Combine all book chunks
   - Feed to Path C (Deep Reasoner) with synthesis instructions
   - Prompt: "You have context from X different books. Explicitly contrast them. Create a table comparing Book A vs Book B."

**Keywords That Trigger Map-Reduce:**
```python
["compare", "comparison", "contrast", "difference between", "similarities between"]
```

**Files Modified:**
- `backend/app/routers/chat.py`: Added Map-Reduce logic in Path C, multi-book synthesis prompt

---

## üöß IN PROGRESS / NEEDS COMPLETION

### 3. Streaming (The "Waiting Room" Problem) ‚è≥

**Status:** Partially Implemented - Backend structure added, needs completion

**What's Needed:**

#### Backend (Streaming Endpoint):
1. Create `/stream` endpoint that uses `stream=True` in OpenAI calls
2. Send thinking steps as SSE events:
   - `{"type": "thinking", "step": "Searching hybrid index..."}`
   - `{"type": "thinking", "step": "Consulting Deep Reasoner..."}`
   - `{"type": "thinking", "step": "Retrieved 15 chunks from 2 books"}`
3. Stream tokens as they arrive:
   - `{"type": "token", "content": "According"}`
   - `{"type": "token", "content": " to"}`
   - `{"type": "token", "content": " #chk_a1b2c3d4"}`
4. Send final metadata:
   - `{"type": "done", "sources": [...], "chunk_map": {...}, "tokens_used": 123}`

#### Frontend (Streaming UI):
1. Show thinking steps during search phase:
   - "üîç Searching hybrid index..."
   - "üß† Consulting Deep Reasoner..."
   - "üìö Retrieved 15 chunks from 2 books"
2. Stream tokens token-by-token as they arrive
3. Real-time citation parsing: As soon as `/#chk_[a-f0-9]{8}/` pattern appears in buffer:
   - Immediately replace with `<CitationBadge />` component
   - Makes UI feel "alive" and clickable instantly
4. Update message state in real-time (not waiting for complete response)

**Current Status:**
- ‚úÖ Backend imports added (`StreamingResponse`, `json`, `asyncio`)
- ‚ùå Streaming endpoint not yet implemented
- ‚ùå Frontend streaming not yet implemented

---

## üìã NEXT STEPS

### Priority 1: Complete Streaming Implementation

1. **Backend Streaming Endpoint:**
   - Create `/stream` route that mirrors `/chat` logic but with streaming
   - Send thinking steps before OpenAI call
   - Use `stream=True` in OpenAI calls
   - Yield tokens as they arrive
   - Save final message to database after streaming completes

2. **Frontend Streaming Handler:**
   - Update `QuantumChat.tsx` to use streaming endpoint
   - Add thinking steps UI component
   - Implement token-by-token rendering
   - Real-time citation parsing with regex hook

### Priority 2: Testing & Refinement

1. Test conversation memory with pronoun resolution
2. Test Map-Reduce with multi-book compare queries
3. Test streaming with all three paths (A, B, C)
4. Performance testing for streaming overhead

---

## üîç VERIFICATION CHECKLIST

### Conversation Memory ‚úÖ
- [x] Fetches last 3 turn pairs from database
- [x] Query rewrite using GPT-4o-mini
- [x] Rewritten query used for embedding generation
- [x] Rewritten query used for keyword search
- [x] Conversation history injected into all three paths (A, B, C)
- [ ] Test: "What is the bond maturity?" ‚Üí "What happens if we miss that date?" should resolve "that date"

### Map-Reduce ‚úÖ
- [x] Detects multi-book compare queries
- [x] Sequential searches per book (top 5 chunks)
- [x] Combines chunks for synthesis
- [x] Multi-book synthesis prompt with table format suggestion
- [ ] Test: "Compare risk factors in Book A vs Book B" should use Map-Reduce

### Streaming ‚è≥
- [ ] Streaming endpoint created
- [ ] Thinking steps sent as SSE events
- [ ] Tokens streamed token-by-token
- [ ] Frontend receives and displays thinking steps
- [ ] Frontend streams tokens in real-time
- [ ] Citations parsed and rendered as they arrive
- [ ] Final metadata sent after streaming completes

---

## üìù IMPLEMENTATION NOTES

### Conversation Memory Implementation Details:

**Query Rewrite Prompt:**
```
You are a query rewriting assistant. Your job is to rewrite user questions by resolving pronouns and contextual references based on conversation history.

Conversation History:
{history_text}

Current User Question: {user_message}

Rewrite the question by:
1. Replacing pronouns (this, that, it, them) with specific entities mentioned in history
2. Expanding abbreviations to full terms
3. Clarifying ambiguous references
4. Making the question self-contained and searchable
```

**History Injection Format:**
```
Previous conversation context:
User: What is the bond maturity?
Assistant: The bond maturity is October 15, 2025.

User: What happens if we miss that date?
```

### Map-Reduce Implementation Details:

**Synthesis Prompt Addition:**
```
MULTI-BOOK SYNTHESIS (MAP-REDUCE):
You have retrieved chunks from 2 different books: Book A, Book B
- Explicitly contrast information across books
- Create clear comparisons between different sources
- Use table format if comparing structured data (e.g., "Book A: X | Book B: Y")
- Identify commonalities and differences between sources
- Cite which book each piece of information comes from using #chk_xxx citations
```

### Streaming Implementation Plan:

**Backend Event Types:**
- `thinking`: "Searching hybrid index...", "Consulting Deep Reasoner..."
- `token`: Individual tokens from OpenAI stream
- `citation`: Citation detected (for immediate rendering)
- `done`: Streaming complete with metadata

**Frontend Hook:**
```typescript
useEffect(() => {
  const citationRegex = /#chk_[a-f0-9]{8}/gi
  // As stream buffer updates, scan for citations
  // Replace with <CitationBadge /> immediately
}, [streamBuffer])
```

---

## üêõ KNOWN ISSUES / TODO

1. **Path B Conversation History:** Path B (Summaries) doesn't need query rewrite (uses pre-computed summaries), but conversation history is still injected ‚úÖ
2. **Path C Scope Issue:** Fixed `book_chunks_map` scope check ‚úÖ
3. **Streaming Performance:** Need to test streaming overhead vs non-streaming
4. **Citation Parsing Edge Cases:** What if citation is split across multiple tokens? (e.g., "#chk_" arrives, then "a1b2c3d4" later)
5. **Error Handling in Streaming:** What happens if streaming fails mid-response?
